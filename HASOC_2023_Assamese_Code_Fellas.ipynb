{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hate Speech Detection on Assamese - HASOC 2023\n",
        "\n",
        "## Team: Code Fellas\n",
        "- Members: Abhinav, Adarsh, Ananya, Dinesh\n",
        "\n",
        "In the HASOC 2023 competition, our team \"Code Fellas\" took on the challenge of hate speech detection in the Assamese language. We employed a variety of approaches, ranging from basic machine learning models to more advanced deep learning techniques.\n",
        "\n",
        "### Approaches Explored:\n",
        "\n",
        "1. **Traditional Models:**\n",
        "   - Logistic Regression\n",
        "   - Support Vector Machine (SVM)\n",
        "   - XGBoost\n",
        "   - Decision Trees\n",
        "\n",
        "2. **Deep Learning Models:**\n",
        "   - LSTM (Long Short-Term Memory)\n",
        "   - BiLSTM (Bidirectional LSTM)\n",
        "   - LSTM with CNN 1D\n",
        "   - BiLSTM with CNN 1D\n",
        "   - XLM Roberta\n",
        "   - M-Bert (Cased and Uncased)\n",
        "   - M-Roberta\n",
        "   - Assamese Bert\n",
        "   - Distilled Bert\n",
        "   - Indic Bert\n",
        "   \n",
        "### Results:\n",
        "\n",
        "After rigorous experimentation, we found that the IndicBERT model yielded the best accuracy for hate speech detection in Assamese, based on our research. The model achieved an impressive F1 Score of 0.69726, showcasing its effectiveness in handling the nuances of the Assamese language and detecting hate speech accurately.\n",
        "\n",
        "Our journey in this competition allowed us to delve into the complexities of hate speech detection, explore a wide range of models, and understand their strengths and weaknesses in the context of Assamese text.\n",
        "\n",
        "We're proud of our team's collaborative efforts and the achievements we've made in advancing the field of hate speech detection for the Assamese language. We look forward to future opportunities to contribute to such meaningful tasks."
      ],
      "metadata": {
        "id": "KSyzx42wwX2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NA2HhO65UAxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "ukIUXfhOB_a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oothbbufpz8p"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "k1Bd-kMxp55-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Loading the train and test data. We are also calling the model name here"
      ],
      "metadata": {
        "id": "jSxIvOx4Ns-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN = '/content/train_A_AH_HASOC2023.csv'\n",
        "TEST = '/content/test_A_AH_HASOC2023.csv'\n",
        "MAPPER = ['text', 'task_1'] # [X, Y]\n",
        "# MODEL = 'bert-base-multilingual-cased'\n",
        "# MODEL = 'bert-base-multilingual-uncased'\n",
        "# MODEL = 'unitary/multilingual-toxic-xlm-roberta'\n",
        "# MODEL = 'l3cube-pune/assamese-bert'\n",
        "# MODEL = 'xlm-mlm-100-1280'\n",
        "# MODEL = 'distilbert-base-multilingual-cased'\n",
        "MODEL = 'ai4bharat/indic-bert'\n",
        "NUM_LABELS = 2"
      ],
      "metadata": {
        "id": "Nq-bvIjiynd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Splitting the train dataset into train and validation data. First we would take the test size as 0.2. Later on, we would take the overall training data"
      ],
      "metadata": {
        "id": "_iXea7Y1N_Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train_df = pd.read_csv(TRAIN)\n",
        "\n",
        "train_df[MAPPER[1]] = train_df[MAPPER[1]].map({'NOT': 0, 'HOF': 1})\n",
        "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42,stratify=train_df[\"task_1\"])"
      ],
      "metadata": {
        "id": "WqxGLQAByluS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "Ow7cEeABmlxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. We will import the tokenizer and the model required to run our model"
      ],
      "metadata": {
        "id": "6WEmL087v1Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer   # Importing autotokenizer and automodel from transformers for indicbert model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)    # Creating the tokenizer\n",
        "model = AutoModel.from_pretrained(MODEL, num_labels=NUM_LABELS) #Creating the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')     # We will use inbuilt cuda gpu if available. Else we would use cpu."
      ],
      "metadata": {
        "id": "4CPC86ag2ver"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocess the data. We would do tokenization and convert our data into pytorch tensors"
      ],
      "metadata": {
        "id": "jkpLB-zZv-k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    inputs = tokenizer(df[MAPPER[0]].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
        "    labels = torch.tensor(df[MAPPER[1]].tolist())\n",
        "    return inputs, labels\n",
        "\n",
        "train_inputs, train_labels = preprocess_data(train_df)   # Applying the tokenizer and converting the inputs into tensors.\n",
        "test_inputs, test_labels = preprocess_data(test_df)  # No labels for test set"
      ],
      "metadata": {
        "id": "mQWCYAm3x_F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 96     # Taking the batch size as 96\n",
        "\n",
        "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "83ilafpVyH-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Defining our optimizer"
      ],
      "metadata": {
        "id": "-FQ2rb0PwKnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)    # we will apply AdamW optimizer with exponential learning rate taking initial value as 10^-5 and gamma as 0.9\n",
        "#decay_rate = 0.9\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
      ],
      "metadata": {
        "id": "Gjx5Nkd2yLOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training Phase of the Model"
      ],
      "metadata": {
        "id": "b0WNU4J-z7rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30 # *** setting the number of epochs to 30 initially. Later on, based on best epoch, we update this value. ***\n",
        "\n",
        "PATIENCE = 2 #setting the patience value to 2 which means when ever the loss increases continuously two times on the validation data then the training will be halted automatically\n",
        "best_val_loss = float('inf') #defining int max as the best_val_loss till now before the start of the training of the model\n",
        "early_stopping_counter = 0 #early_stopping_counter is the variable which counts the number of times validation loss has been increased\n",
        "\n",
        "#epoch wise training of the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() #entering the training mode for model as per pytorch\n",
        "    total_loss = 0 #initializing the loss before the epoch to 0 and it is used to get the accumilated overall loss for all the batches\n",
        "\n",
        "    #training the model batchwise as per the defined batch size\n",
        "    for batch in train_loader: #running a loop to train all the train batches\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device) #The input tensors (input_ids, attention_mask, labels) are moved to the device (presumably a GPU) using .to(device).\n",
        "\n",
        "        optimizer.zero_grad() #clearing the gradients of the model parameters for each batch as we are calculating the gradients for individual batches\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,labels=labels)#The model is called with the inputs, and it returns outputs which includes the loss.\n",
        "        loss = outputs.loss\n",
        "        loss.backward()#Backpropagation is performed by calling loss.backward(), computing the gradients of the loss with respect to the model's parameters.\n",
        "        optimizer.step() #The optimizer is updated using optimizer.step() to modify the model's parameters based on the computed gradients.\n",
        "\n",
        "        total_loss += loss.item()#accumulating the loss of each batch in the total loss\n",
        "    model.eval() #setting the model to evaluation mode correctly\n",
        "    predictions = [] #list to store the predictions to get the evaluation loss\n",
        "    val_loss = 0 #initiating the val loss to zero\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader: #The code iterates through each batch in the test_loader (presumably the validation dataset).\n",
        "            input_ids, attention_mask,labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device) #moving the tensors to the device\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels) #gettting the outputs by running the model\n",
        "            val_loss += outputs.loss.item() #getting the final validation accuracy after one complete epoch\n",
        "\n",
        "    #scheduler.step()\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0 #if the validation loss decreases then we will make the early stopping counter 0\n",
        "    else:\n",
        "        early_stopping_counter += 1 #at each and every epoch if the validation loss gets increasing we will increase teh early stopping counter by the value of 1\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)},val_loss: {val_loss / len(test_loader)}') #printing the loss of each epoch\n",
        "    if early_stopping_counter >= PATIENCE: #if the count of early stopping crosses the patience level then we will exit the training part\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ],
      "metadata": {
        "id": "KTZGeNNayORx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We will check the epoch upto which both training loss and validation loss got decreased. We will consider that epoch and re run the model at that epoch. This is because to make sure our model doesn't get overfit nor it get's underfitted.\n",
        "\n",
        "## Here, it got stopped at 6th epoch and we saw 4th epoch is the best one."
      ],
      "metadata": {
        "id": "Q0BCrKQoSqEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluation of the test data(from train test split) and predictions phase"
      ],
      "metadata": {
        "id": "KnqRLMSr0g74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report # importing the classification report from sklearn to predict the scores of the model\n",
        "model.eval() #setting the model to evaluation mode\n",
        "predictions = [] #initiating an empty list of predictions\n",
        "truths = [] #initiating an empty list of original values(labels)\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader: #testing the model on the test data batch wise\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device) #loading the batches to device\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)# getting the predictions from the model\n",
        "        logits = outputs.logits\n",
        "        predicted_labels = torch.argmax(logits, dim=1)#getting the labels from the outputs predicted\n",
        "        truths.extend(labels.cpu().tolist())# updating the original labels to device\n",
        "        predictions.extend(predicted_labels.cpu().tolist()) # updating the predicted labels to device\n",
        "\n",
        "temp = predictions\n",
        "print(classification_report(truths,temp)) # printing the classification report based on the original and predicted labels\n",
        "# test_df['predicted_label'] = predictions"
      ],
      "metadata": {
        "id": "Zvwajy7EEEIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Make predictions from the original Test Data (Given test data)"
      ],
      "metadata": {
        "id": "fv3gI4I80pvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL) #loading  the tokenizer\n",
        "# model = BertForSequenceClassification.from_pretrained(MODEL, num_labels=NUM_LABELS)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #getting the device\n",
        "test_df = pd.read_csv(\"test_A_AH_HASOC2023.csv\") # reading the original test data\n",
        "#function to preprocess the data and tokenize the data\n",
        "def preprocess_data(df):\n",
        "    inputs = tokenizer(df[MAPPER[0]].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
        "    return inputs\n",
        "\n",
        "\n",
        "test_inputs = preprocess_data(test_df)  # preprocess the test data\n",
        "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'])\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) #loading the data into the dataloader which gives the data batchwise"
      ],
      "metadata": {
        "id": "eBMRjbj3XUu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()# setting the model in evaluation mode\n",
        "predictions = [] #initializing the empty predictions list\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader: #for each batch in test_loader data\n",
        "        input_ids, attention_mask = batch\n",
        "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device) #binding the batch to the device\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask) #getting the predictions from the model\n",
        "        logits = outputs.logits\n",
        "        predicted_labels = torch.argmax(logits, dim=1) #getting the predicted labels from the predictions made\n",
        "        predictions.extend(predicted_labels.cpu().tolist())\n",
        "\n",
        "test_df['predicted_label'] = predictions #appending the predictions of the model to the test data frame"
      ],
      "metadata": {
        "id": "PBnHXA2myUUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df #viewing the resulting test dataframe"
      ],
      "metadata": {
        "id": "9xt6ZyCQ0MgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"predicted_label\"].value_counts()#verifying the value_counts of the predictions to check any high overfitting"
      ],
      "metadata": {
        "id": "Ce0m3W4CYq8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = test_df.drop(\"text\",axis=1)# dropping the unwanted axis"
      ],
      "metadata": {
        "id": "Vnfjzj1hYwCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "DlWOUDxPY37H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[MAPPER[1]] = test_df[\"predicted_label\"].map({0: 'NOT', 1: 'HOF'}) #reverse mapping the binary labels to the original text labels"
      ],
      "metadata": {
        "id": "UHE3sAs-Y49P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "Hv1wI5_HZFS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = test_df.drop(\"predicted_label\",axis=1)"
      ],
      "metadata": {
        "id": "hIjT5aJmZJ3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "21p1S9JVZPyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Saving the predictions to the file"
      ],
      "metadata": {
        "id": "F-EgeepM1oAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv(\"final_assamese_l3cube-pune_assamese-bert_epoch_7.csv\",index=False) #saving the predictions dataframe to csv file"
      ],
      "metadata": {
        "id": "aIHHJ_IvZQwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head(2)"
      ],
      "metadata": {
        "id": "qRs31d4FZVps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}